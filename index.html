<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions">
  <meta name="keywords" content="BranchOut, autonomous driving, diffusion, GMM, multimodality, motion planning, CoRL 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,700|Castoro:400,700" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="shortcut icon" href="./logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:site_name" content="BranchOut" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions" />
  <meta property="og:description" content="A GMM-based diffusion planner and a human-in-the-loop multimodal benchmark for autonomous driving decisions." />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions" />
  <meta name="twitter:description" content="A GMM-based diffusion planner and a human-in-the-loop multimodal benchmark for autonomous driving decisions." />

  <script src="https://www.youtube.com/iframe_api"></script>
  <style>
    .top-pad { padding-top: 1rem; }
    .hero-title { letter-spacing: 0.2px; }
    .author-block a { margin-right: 0.2rem; }
    .metric-card { border: 1px solid #eee; border-radius: 12px; padding: 1rem; }
    .center-img { display: block; margin: 0 auto; }
    .brmod::before { content: "\A"; white-space: pre; }
    .pub-links .button { margin: 0 .25rem; }
    .footer a.bottom_buttons { margin: 0 .5rem; }
    .is-tight p { margin-bottom: .6rem; }
    pre code { font-size: .9rem; }
  </style>
</head>

<body>
  <!-- HERO -->
  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title hero-title">BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://hee-jae-kim.github.io/" target="_blank" rel="noopener">Hee Jae Kim</a> &emsp;
                <a href="https://zekai-yin.github.io/" target="_blank" rel="noopener">Zekai Yin</a> &emsp;
                <a href="https://leilai125.github.io/" target="_blank" rel="noopener">Lei Lai</a> &emsp;
                <a href="#" target="_blank" rel="noopener">Jason Lee</a> &emsp;
                <a href="https://eshed1.github.io/" target="_blank" rel="noopener">Eshed Ohn-Bar</a>
                <br/>Boston University
                <span class="brmod"></span>CoRL 2025
              </span>
            </div>

            <div class="column has-text-centered top-pad">
              <div class="publication-links pub-links">
                <!-- Paper (local copy) -->
                <span class="link-block">
                  <a href="./55_BranchOut_Capturing_Realist.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper (PDF)</span>
                  </a>
                </span>
                <!-- Video anchor -->
                <span class="link-block">
                  <a href="#method_video" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-youtube"></i></span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code (placeholder) -->
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark" aria-disabled="true">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
                <!-- Poster (optional placeholder) -->
                <!-- <span class="link-block">
                  <a href="./resources/BranchOut_Poster.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>Poster</span>
                  </a>
                </span> -->
              </div>
            </div>

            <!-- At-a-glance metrics -->
            <div class="columns is-centered top-pad">
              <div class="column is-two-thirds">
                <div class="columns is-multiline">
                  <div class="column is-one-third">
                    <div class="metric-card has-text-centered">
                      <p class="heading">nuScenes (Open-loop)</p>
                      <p class="title is-5">L2@3s: <b>1.41 m</b></p>
                      <p class="subtitle is-7">Avg L2: 0.83 m; Fréchet: 2.29; NLL: 3.72; JSD: 0.36</p>
                    </div>
                  </div>
                  <div class="column is-one-third">
                    <div class="metric-card has-text-centered">
                      <p class="heading">HUGSIM (Closed-loop)</p>
                      <p class="title is-5">HD-Score: <b>0.47</b></p>
                      <p class="subtitle is-7">NC: 0.76; DAC: 0.99; TTC: 0.69; COM: 1.00; Rc: 0.58</p>
                    </div>
                  </div>
                  <div class="column is-one-third">
                    <div class="metric-card has-text-centered">
                      <p class="heading">Multimodal GT Realism</p>
                      <p class="title is-5">3s L2: <b>0.79 m</b></p>
                      <p class="subtitle is-7">Fréchet: 1.46; NLL: 3.48 (photorealistic sim)</p>
                    </div>
                  </div>
                </div>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ABSTRACT + VIDEO -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified is-tight">
            <p>
              Modeling the nuanced, multimodal nature of human driving remains a core challenge for autonomous systems, as existing methods often fail to capture the diversity of plausible behaviors in complex real-world scenarios. We introduce an end-to-end planner and a benchmark for modeling realistic multimodality in autonomous driving decisions. Our Gaussian Mixture Model (GMM)-based diffusion planner explicitly captures human-like, multimodal driving decisions in diverse contexts. While the model achieves state-of-the-art performance on current benchmarks, it also reveals weaknesses in standard evaluation practices that rely on single ground-truth trajectories or coarse closed-loop metrics and inadvertently penalize diverse yet plausible alternatives. We further develop a human-in-the-loop simulation benchmark that enables finer-grained evaluation and measures multimodal realism in challenging driving settings.
            </p>
          </div>
        </div>
      </div>

      <div id="method_video" class="columns is-centered has-text-centered top-pad">
        <div class="column is-half">
          <h2 class="title is-3">Overview Video</h2>
          <div class="publication-video">
            <video controls>
              <source src="./resources/highlight_video.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOTIVATION / HIGHLIGHTS -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Motivation &amp; Contributions</h2>
          <div class="content has-text-justified is-tight">
            <p>
              Real-world driving involves multiple plausible and safe decisions in a given scenario. Yet, many current planners are deterministic or exhibit mode collapse, and most datasets provide only a single ground truth per scene—penalizing valid alternatives. BranchOut advances both modeling and evaluation of <em>realistic multimodality</em> in planning.
            </p>
            <ul>
              <li><b>GMM-based Diffusion Planner.</b> An end-to-end, scene-conditioned diffusion model with a branched GMM head that outputs multiple trajectory hypotheses per high-level command (Left/Straight/Right), improving multimodal coverage and realism.</li>
              <li><b>Human-in-the-Loop Multimodal Benchmark.</b> A photorealistic, closed-loop re-driving framework that collects diverse, human trajectories per scene—validated against real-world logs and complemented by a virtual digital-twin setup.</li>
              <li><b>State-of-the-art Results.</b> Strong open-loop and closed-loop performance, and improved distributional alignment (NLL/JSD), while using compact sampling (one sample per command).</li>
            </ul>
          </div>
          <img src="./resources/motivation.png" alt="Multimodal trajectory illustration" class="center-img" />
        </div>
      </div>
    </div>
  </section>

  <!-- METHOD -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified is-tight">
            <p>
              BranchOut follows a standard end-to-end planning setup: a scene context comprises six multi-view camera images and an HD map. Unlike some prior work, we do not use ego-status or past ego trajectory inputs by default. A transformer denoiser leverages multi-head cross-attention (MHCA) to condition ego-trajectory queries on scene features. A <b>branched GMM head</b>, indexed by a discrete high-level command c ∈ {Left, Straight, Right}, predicts K mean trajectories and mixture weights per command, enabling selection among plausible futures.
            </p>
            <p>
              We train with a diffusion objective and an additional negative log-likelihood over the predicted multimodal distributions, plus safety constraints: L = L<sub>plan</sub> + λ<sub>NLL</sub>L<sub>NLL</sub> + λ<sub>c</sub>L<sub>constraints</sub> (λ<sub>NLL</sub>=0.1, λ<sub>c</sub>=0.1). At inference, we initialize from Gaussian noise and solve the reverse ODE with a fast single-step DPM-Solver++ sampler.
            </p>
          </div>
          <img src="./resources/arch.png" width="80%" alt="Architecture" class="center-img" />
        </div>
      </div>

      <div class="columns is-centered has-text-centered top-pad">
        <div class="column is-four-fifths">
          <h3 class="title is-4" align="left">Human-in-the-Loop Simulation &amp; Multimodal GT</h3>
          <div class="content has-text-justified is-tight">
            <p>
              We augment a photorealistic reconstruction-based simulator with an interactive re-driving interface, a kinematic bicycle model for ego motion, and collision detection using depth from reconstruction. Participants re-drive each scene multiple times (five per participant), producing a <em>set</em> of realistic, diverse trajectories per scenario. We validate realism by comparing to logged real-world trajectories; a virtual digital-twin environment provides an additional reference.
            </p>
          </div>
          <img src="./resources/figure1_crop.png" width="80%" alt="Human-in-the-loop simulation" class="center-img" />
        </div>
      </div>
    </div>
  </section>

  <!-- EVALUATION / RESULTS -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Evaluation &amp; Results</h2>
          <div class="content has-text-justified is-tight">
            <p>
              <b>Setup &amp; Metrics.</b> We report L2 displacement error against the single GT provided by nuScenes, and introduce a multimodal evaluation using 16 GT trajectories per scene (15 from our benchmark + 1 from nuScenes) with minimum Fréchet distance. We further assess distributional quality via Negative Log-Likelihood (NLL) and Speed Jensen–Shannon Divergence (JSD). For closed-loop, we follow HUGSIM with No Collision (NC), Drivable Area Compliance (DAC), Time to Collision (TTC), Comfort (COM), Route Completion (Rc), and the composite HD-Score.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered top-pad">
        <div class="column is-four-fifths">
          <h3 class="title is-4" align="left">Open-Loop (nuScenes, Single &amp; Multimodal GT)</h3>
          <div class="content has-text-justified is-tight">
            <p>
              BranchOut achieves <b>L2@3s = 1.41 m</b> (Avg 0.83 m), outperforming prior multimodal planners with compact sampling (one sample per command). Under multimodal metrics, BranchOut yields <b>Fréchet = 2.29</b>, <b>NLL = 3.72</b>, and <b>JSD = 0.36</b>, reflecting better coverage and human-aligned diversity.
            </p>
          </div>
          <img src="./resources/result_openloop.png" width="80%" alt="Open-loop results" class="center-img" />
        </div>
      </div>

      <div class="columns is-centered has-text-centered top-pad">
        <div class="column is-four-fifths">
          <h3 class="title is-4" align="left">Closed-Loop (HUGSIM)</h3>
          <div class="content has-text-justified is-tight">
            <p>
              BranchOut attains the best overall <b>HD-Score = 0.47</b>, with strong safety and progress: NC 0.76, DAC 0.99, TTC 0.69, COM 1.00, Rc 0.58—substantially higher route completion than unimodal baselines.
            </p>
          </div>
          <img src="./resources/result_closedloop.png" width="80%" alt="Closed-loop results" class="center-img" />
        </div>
      </div>

      <div class="columns is-centered has-text-centered top-pad">
        <div class="column is-four-fifths">
          <h3 class="title is-4" align="left">Qualitative Examples</h3>
          <div class="content has-text-justified is-tight">
            <p>
              Visualizations show BranchOut capturing subtle interactions (vehicle–vehicle, human–vehicle) and selecting among plausible maneuvers (e.g., varying headways or offsets around obstacles) while preserving safety.
            </p>
          </div>
          <img src="./resources/qaulitative_example.png" width="80%" alt="Qualitative examples" class="center-img" />
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX -->
  <section class="section" id="bibtex">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">BibTeX</h2>
          <div class="content has-text-justified">
<pre><code>@inproceedings{kim2025branchout,
  title     = {BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions},
  author    = {Kim, Hee Jae and Yin, Zekai and Lai, Lei and Lee, Jason and Ohn-Bar, Eshed},
  booktitle = {Proceedings of the 9th Conference on Robot Learning (CoRL)},
  year      = {2025}
}</code></pre>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ACKS -->
  <section class="section" id="acks">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Acknowledgments</h2>
          <div class="content has-text-centered">
            We thank the Red Hat Collaboratory (award #2024-01-RH07, #2025-01-RH04) and the National Science Foundation (IIS-2152077) for supporting this research.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- FOOTER -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a href="./55_BranchOut_Capturing_Realist.pdf" class="large-font bottom_buttons">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a href="#" class="large-font bottom_buttons" aria-disabled="true">
          <i class="fab fa-github"></i>
        </a>
        <br/>
        <p>Page template adapted from <a href="https://nerfies.github.io/" target="_blank" rel="noopener"><span class="dnerf">D-NeRF</span></a>, <a href="https://worldsheet.github.io/" target="_blank" rel="noopener"><span>Worldsheet</span></a>, and <a href="https://zlai0.github.io/VideoAutoencoder/" target="_blank" rel="noopener"><span>Video Autoencoder</span></a>.</p>
      </div>
    </div>
  </footer>
</body>
</html>
